{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbgz49PvHhLt"
      },
      "source": [
        "# 심화과제 - Pre-trained 모델로 효율적인 NLP 모델 학습하기\n",
        "\n",
        "# Multi-genre natural language inference(MNLI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1LqgujQUbv6X",
        "outputId": "8c0473f9-059b-498f-a5c5-273be7566a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/139.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/897.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU tqdm boto3 requests regex sentencepiece sacremoses datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_LQlvLxptgB"
      },
      "source": [
        "# dataset - kaggle 셋팅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dsKkrtop1W7",
        "outputId": "16104a36-066f-4670-d626-87cd287f1912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/unlocking-language-understanding-with-the-multin\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"thedevastator/unlocking-language-understanding-with-the-multin\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3uzexaC9p52a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def load_data(path, nrows=None):\n",
        "  df = pd.read_csv(path, nrows=nrows, keep_default_na=False)\n",
        "  data = []\n",
        "  for _, row in df.iterrows():\n",
        "    if len(row['premise']) * len(row['hypothesis']) != 0:\n",
        "      data.append({'premise': row['premise'], 'hypothesis': row['hypothesis'], 'label': row['label']})\n",
        "\n",
        "  return data\n",
        "\n",
        "# 경로 : load_data(path + '/불러올파일명.csv', nrows=1000)\n",
        "train_data = load_data(path+'/train.csv', nrows=1000)\n",
        "test_data = load_data(path+'/validation_matched.csv', nrows=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRbgKWMMyCD6",
        "outputId": "fda5f33c-0ec3-4165-c174-0f21dd95e5a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.',\n",
              "  'hypothesis': 'Product and geography are what make cream skimming work. ',\n",
              "  'label': 1},\n",
              " {'premise': 'The new rights are nice enough',\n",
              "  'hypothesis': 'Everyone really likes the newest benefits ',\n",
              "  'label': 1})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data[0], test_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYaL7XcHqUrE"
      },
      "source": [
        "# Pre-trained 모델 선정\n",
        "\n",
        "MNLI 단순한 언어 이해 능력만을 필요하기 때문에\n",
        "DistilBERT pre-training 때 사용한 tokenizer를 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXI7kZW4qfu2",
        "outputId": "d3312b31-b955-47f1-9306-48887cae18ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# DistilBERT 모델용 tokenizer 로드 (pretrained)\n",
        "# 이 tokenizer는 문장을 토큰화해서 모델이 이해할 수 있는 input_ids로 변환해줌\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvfl_uFLIMWO"
      },
      "source": [
        "# collate_fn 코드 수정\n",
        "MNLI는 두개의 문장 (premise, hypothesis)을 처리해야함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rE-y8sY9HuwP"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 데이터를 배치로 묶기 위한 함수 정의\n",
        "def collate_fn(batch):\n",
        "    max_len = 400  # 입력 문장의 최대 길이 설정\n",
        "    texts, labels = [], []\n",
        "\n",
        "    # 배치 내 각 샘플에 대해 premise와 hypothesis 추출\n",
        "    for row in batch:\n",
        "        labels.append(row['label'])\n",
        "        premise = row['premise']\n",
        "        hypothesis = row['hypothesis']\n",
        "        texts.append(premise + hypothesis)  # 두 문장을 연결\n",
        "\n",
        "    # tokenizer로 텍스트를 토큰화하고, 최대 길이로 패딩 및 자르기\n",
        "    # tokenizer는 사전에 정의되어 있어야 함 (예: tokenizer = AutoTokenizer.from_pretrained(...))\n",
        "    texts = torch.LongTensor(\n",
        "        tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids\n",
        "    )\n",
        "\n",
        "    # 라벨 리스트를 LongTensor로 변환\n",
        "    labels = torch.LongTensor(labels)\n",
        "\n",
        "    # 모델 학습에 필요한 입력 (토큰화된 문장들)과 정답 라벨 반환\n",
        "    return texts, labels\n",
        "\n",
        "# 학습용 DataLoader 정의 (shuffle=True로 배치 순서 랜덤화)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# 테스트용 DataLoader 정의 (shuffle=False로 배치 순서 고정)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW7ETZQzzNp2",
        "outputId": "a38ff759-16e0-4f98-ff09-8f983986c093"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# 텍스트 분류 모델 정의 (DistilBERT + Linear layer)\n",
        "class TextClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # 사전학습된 DistilBERT 모델을 encoder로 불러옴 (pretrained transformer)\n",
        "        self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "\n",
        "\n",
        "        # [CLS] 토큰 분류기 정의 -> 3\n",
        "        self.classifier = nn.Linear(768, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder에 input_ids 전달\n",
        "        x = self.encoder(x)['last_hidden_state']\n",
        "\n",
        "        # [CLS] 토큰 위치 벡터를 classification head에 전달\n",
        "        x = self.classifier(x[:, 0])\n",
        "\n",
        "        return x  # logit 출력\n",
        "\n",
        "model = TextClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "uyTciaPZ0KYo"
      },
      "outputs": [],
      "source": [
        "for param in model.encoder.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvvaAEwCznt-",
        "outputId": "a2c2c0df-2c74-4077-d506-ceb4ffc0eecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0 | Train Loss: 33.141939997673035\n",
            "Epoch   1 | Train Loss: 32.815880954265594\n",
            "Epoch   2 | Train Loss: 32.723875999450684\n",
            "Epoch   3 | Train Loss: 32.60914957523346\n",
            "Epoch   4 | Train Loss: 32.16609215736389\n",
            "Epoch   5 | Train Loss: 32.44495189189911\n",
            "Epoch   6 | Train Loss: 32.10079514980316\n",
            "Epoch   7 | Train Loss: 31.958671510219574\n",
            "Epoch   8 | Train Loss: 32.27595442533493\n",
            "Epoch   9 | Train Loss: 31.909388661384583\n",
            "Epoch  10 | Train Loss: 31.57521915435791\n",
            "Epoch  11 | Train Loss: 31.869503498077393\n",
            "Epoch  12 | Train Loss: 31.935392558574677\n",
            "Epoch  13 | Train Loss: 31.58659189939499\n",
            "Epoch  14 | Train Loss: 31.388736367225647\n",
            "Epoch  15 | Train Loss: 31.253267288208008\n",
            "Epoch  16 | Train Loss: 31.50137084722519\n",
            "Epoch  17 | Train Loss: 31.519350171089172\n",
            "Epoch  18 | Train Loss: 31.311498403549194\n",
            "Epoch  19 | Train Loss: 31.132458209991455\n",
            "Epoch  20 | Train Loss: 31.08728277683258\n",
            "Epoch  21 | Train Loss: 30.67995649576187\n",
            "Epoch  22 | Train Loss: 30.599449813365936\n",
            "Epoch  23 | Train Loss: 30.72466343641281\n",
            "Epoch  24 | Train Loss: 30.828125774860382\n",
            "Epoch  25 | Train Loss: 30.646695733070374\n",
            "Epoch  26 | Train Loss: 30.699687063694\n",
            "Epoch  27 | Train Loss: 30.48652547597885\n",
            "Epoch  28 | Train Loss: 30.919901311397552\n",
            "Epoch  29 | Train Loss: 30.62693816423416\n",
            "Epoch  30 | Train Loss: 30.14224636554718\n",
            "Epoch  31 | Train Loss: 30.1253879070282\n",
            "Epoch  32 | Train Loss: 30.3101669549942\n",
            "Epoch  33 | Train Loss: 30.17417573928833\n",
            "Epoch  34 | Train Loss: 29.992780923843384\n",
            "Epoch  35 | Train Loss: 29.928197503089905\n",
            "Epoch  36 | Train Loss: 30.04700857400894\n",
            "Epoch  37 | Train Loss: 30.20132690668106\n",
            "Epoch  38 | Train Loss: 29.956743001937866\n",
            "Epoch  39 | Train Loss: 30.043096363544464\n",
            "Epoch  40 | Train Loss: 30.32689470052719\n",
            "Epoch  41 | Train Loss: 29.96458351612091\n",
            "Epoch  42 | Train Loss: 30.122779607772827\n",
            "Epoch  43 | Train Loss: 29.470046997070312\n",
            "Epoch  44 | Train Loss: 29.86747694015503\n",
            "Epoch  45 | Train Loss: 29.656333208084106\n",
            "Epoch  46 | Train Loss: 29.62329536676407\n",
            "Epoch  47 | Train Loss: 29.729264795780182\n",
            "Epoch  48 | Train Loss: 29.475443959236145\n",
            "Epoch  49 | Train Loss: 29.93899142742157\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 학습 설정\n",
        "lr = 0.001\n",
        "model = model.to('cuda')  # 모델을 GPU로 이동\n",
        "loss_fn = nn.CrossEntropyLoss() # 다중 분류용 손실 함수로 변경\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "n_epochs = 50\n",
        "\n",
        "# 학습 루프\n",
        "for epoch in range(n_epochs):\n",
        "    total_loss = 0.\n",
        "    model.train()  # 학습 모드 설정\n",
        "\n",
        "    for data in train_loader:\n",
        "        model.zero_grad()  # 이전 gradient 초기화\n",
        "\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda').long()  # GPU 이동 및 long 변환\n",
        "\n",
        "        preds = model(inputs)\n",
        "\n",
        "        loss = loss_fn(preds, labels)  # 손실 계산\n",
        "        loss.backward()  # 역전파\n",
        "        optimizer.step()  # 파라미터 업데이트\n",
        "\n",
        "        total_loss += loss.item()  # loss 누적\n",
        "\n",
        "    print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjphVwXL00E2",
        "outputId": "fd782c4b-ca86-47c3-c4e5-abedeace9cab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========> Train acc: 0.612 | Test acc: 0.426\n"
          ]
        }
      ],
      "source": [
        "def accuracy(model, dataloader):\n",
        "    cnt = 0      # 전체 샘플 수\n",
        "    acc = 0      # 정답 개수 누적\n",
        "\n",
        "    for data in dataloader:\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "        preds = model(inputs)  # 로짓(logit) 출력\n",
        "\n",
        "        # 시그모이드는 생략 가능 (BCEWithLogitsLoss를 썼다면 threshold만 적용)\n",
        "        preds = torch.argmax(preds, dim=-1)\n",
        "\n",
        "        cnt += labels.shape[0]  # 총 샘플 수 누적\n",
        "        acc += (labels == preds).sum().item()  # 예측이 맞은 수 누적\n",
        "\n",
        "    return acc / cnt  # 정확도 반환\n",
        "\n",
        "# 평가 시 gradient 계산 비활성화\n",
        "with torch.no_grad():\n",
        "    model.eval()  # 평가 모드로 전환 (계산 비활성화)\n",
        "    train_acc = accuracy(model, train_loader)\n",
        "    test_acc = accuracy(model, test_loader)\n",
        "\n",
        "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Foks5u95ZQ1_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
